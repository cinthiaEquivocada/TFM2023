{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Codigos para TFM: Cinthia Tavarez Mora"
      ],
      "metadata": {
        "id": "zh9PrfVZUqYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "xjZ8GgCIXkuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvGdF7UIzYDw"
      },
      "outputs": [],
      "source": [
        "#Paquetes a instalar\n",
        "!pip install tweepy\n",
        "!pip3 install twarc\n",
        "!pip install panda\n",
        "!pip install num2words\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6hgoQn-zmK8"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm #para lemmatizacion en español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4w35YJWGr6i"
      },
      "outputs": [],
      "source": [
        "# Paquetes para tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "import glob\n",
        "import tweepy\n",
        "import random\n",
        "import time\n",
        "#import num2words\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recoleccion de datos"
      ],
      "metadata": {
        "id": "9i5i9azVVMbs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOcMdXsyrixX"
      },
      "outputs": [],
      "source": [
        "# PARA UNIFICAR LOS ARCHIVOS DEL 15 DE MARZO AL 31 DE MARZO\n",
        "\n",
        "# Ruta de la carpeta que contiene los archivos TSV comprimidos\n",
        "folder_path = '/content'\n",
        "\n",
        "# Obtener la lista de archivos TSV comprimidos en la carpeta\n",
        "file_list = glob.glob(folder_path + '/*.tsv.gz')\n",
        "\n",
        "# Crear una lista para almacenar los DataFrames de cada archivo\n",
        "dataframes = []\n",
        "\n",
        "# Iterar sobre los archivos TSV comprimidos\n",
        "for file_path in file_list:\n",
        "    with gzip.open(file_path, 'rt') as f:\n",
        "        # Leer el archivo TSV en un DataFrame utilizando pandas\n",
        "        df = pd.read_csv(f, delimiter='\\t')\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenar los DataFrames en uno solo\n",
        "combined_df = pd.concat(dataframes)\n",
        "\n",
        "# Guardar el DataFrame unificado en un archivo TSV\n",
        "combined_df.to_csv('archivo_unificado.tsv', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraccion de datos de Twitter"
      ],
      "metadata": {
        "id": "JUefSugWDymp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izPH9J6RUtTp"
      },
      "outputs": [],
      "source": [
        "# Conexion con la API de Twitter e hidratacion de tweets:\n",
        "# ESTE TOMA EN CUENTA OTROS ASPECTOS A EXTRAER COMO URL, DATE, LOCALIZACION ENTRE OTROS\n",
        "\n",
        "\n",
        "# Configura credenciales de API de Twitter\n",
        "consumer_key = \"colocar\"\n",
        "consumer_secret = \"colocar\"\n",
        "access_token = \"colocar\"\n",
        "access_token_secret = \"colocar\"\n",
        "\n",
        "# Autenticación con API de Twitter\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "# Crear un objeto API\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Archivos de entrada y salida\n",
        "input_file = 'archivo_unificado.tsv'\n",
        "output_file = 'tweets_hydrated1.tsv'\n",
        "\n",
        "# Número máximo de tweets a revisar\n",
        "max_tweets = 1000\n",
        "\n",
        "# Palabra clave para filtrar los tweets\n",
        "keywords = [\"vacunan\",\"antivacuna\",\"trombos\",\"coagulos\",\"imanes\",\"chip\",\"ADN\",\"ARN\",\"manipulan\"]\n",
        "\n",
        "# Leer el archivo de entrada\n",
        "df_input = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "\n",
        "# Cargar el DataFrame existente desde un archivo o variable\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv\") as fd:\n",
        "    df_viejo = pd.read_csv(fd, delimiter=\"\\t\")\n",
        "\n",
        "# Subset con id unicos del archivo viejo\n",
        "existing_ids = df_viejo['User ID']\n",
        "\n",
        "# Filtra los nuevos IDs para evitar duplicados\n",
        "# Filtrar los IDs de los tweets que cumplen las condiciones requeridas\n",
        "filtered_tweet_ids =df_input[(df_input['lang'] == 'es')]['tweet_id']\n",
        "filtered_tweet_ids = [tweet_id for tweet_id in filtered_tweet_ids if tweet_id not in existing_ids]\n",
        "\n",
        "# Obtener los IDs de los tweets únicos del archivo de entrada\n",
        "tweet_ids = filtered_tweet_ids\n",
        "\n",
        "# Seleccionar aleatoriamente los IDs de los tweets a revisar\n",
        "tweet_ids_sample = random.sample(tweet_ids, min(max_tweets, len(tweet_ids)))\n",
        "\n",
        "\n",
        "# Lista para almacenar los datos de los tweets\n",
        "tweet_data = []\n",
        "\n",
        "# Iterar sobre los IDs de los tweets seleccionados aleatoriamente\n",
        "for tweet_id in tweet_ids_sample:\n",
        "    try:\n",
        "        # Obtener el tweet completo a partir del ID\n",
        "        tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
        "\n",
        "        # Verificar si el tweet contiene alguna de las palabras clave\n",
        "        if any(keyword in tweet.full_text for keyword in keywords):\n",
        "            # Obtener la información del tweet\n",
        "            id_user = tweet.user.id_str\n",
        "            name_user = tweet.user.screen_name\n",
        "            location = tweet.user.location\n",
        "            text = tweet.full_text\n",
        "            url = tweet.entities.get('urls')[0]['expanded_url'] if tweet.entities.get('urls') else None,\n",
        "            date = tweet.created_at.date()\n",
        "            hashtags = [tag['text'] for tag in tweet.entities.get('hashtags')]\n",
        "\n",
        "            # Agregar los datos del tweet a la lista\n",
        "            tweet_data.append([tweet_id, id_user, name_user, location, text, url, date, hashtags])\n",
        "\n",
        "    except tweepy.TweepyException as e:\n",
        "        # Manejar errores en caso de que el tweet no pueda ser encontrado o haya algún problema con la API\n",
        "        print(f\"Error al obtener el tweet con ID {tweet_id}: {e}\")\n",
        "\n",
        "# Crear un DataFrame con los datos de los tweets\n",
        "columns = ['Tweet ID', 'User ID', 'User Name', 'Location', 'Text', 'URL', 'Date', 'Hashtags']\n",
        "df_output = pd.DataFrame(tweet_data, columns=columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2K5slmG7Goc"
      },
      "outputs": [],
      "source": [
        "### AQUI hacemos un proceso quizas poco eficiente para guardar los nuevos registros NO DUPLICADOS\n",
        "import os\n",
        "\n",
        "# Cargar el DataFrame existente desde un archivo o variable\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv\") as fd:\n",
        "    df_viejo = pd.read_csv(fd, delimiter=\"\\t\")\n",
        "\n",
        "# Subset con id unicos del archivo viejo\n",
        "existing_ids = df_viejo['User ID']\n",
        "\n",
        "# Filtra los nuevos IDs para evitar duplicados\n",
        "new_ids = df_output[~df_output['User ID'].isin(existing_ids)]\n",
        "\n",
        "# Concatenar los DataFrames existente y nuevo\n",
        "df_combina = pd.concat([df_viejo, new_ids], ignore_index=True)\n",
        "\n",
        "# PARA GUARDAR EL NUEVO ARCHIVO\n",
        "\n",
        "# 1. Creamos la ruta y el nombre de archivo que deseas verificar y eliminar si existe\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "\n",
        "# Verifica si el archivo existe en la ruta especificada\n",
        "if os.path.exists(file_path):\n",
        "    # Elimina el archivo si existe\n",
        "    os.remove(file_path)\n",
        "    print(\"El archivo existente ha sido eliminado.\")\n",
        "else:\n",
        "    print(\"El archivo no existe.\")\n",
        "\n",
        "# NUEVA VEZ le decimos la ruta y el nombre de archivo para guardar el DataFrame combinado en formato TSV\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "\n",
        "# Guarda el DataFrame combinado en el archivo TSV\n",
        "df_combina.to_csv(output_file, sep='\\t', index=False)\n",
        "\n",
        "print(\"Proceso exitoso\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkmeW6HDDI_F"
      },
      "outputs": [],
      "source": [
        "#ALTERNATIVA PARA DEJARLO CORRER SOLO: CREAR UN PROCESO AUTOMATICO PARA QUE CONSULTE Y SUME MAS TUITS AL ARCHIVO FINAL\n",
        "from time import sleep\n",
        " # Número de veces que se ejecutará el código interno\n",
        "num_ejecuciones_interno =2\n",
        "\n",
        "# Intervalo de tiempo en segundos (15 minutos = 900 segundos)\n",
        "intervalo_tiempo = 900\n",
        "\n",
        "# Ciclo externo para repetir el proceso 10 veces\n",
        "for i in range(num_ejecuciones_interno):\n",
        "    # Aquí va tu código\n",
        "\n",
        "    # Configura tus credenciales de API de Twitter\n",
        "    consumer_key = \"colocar\"\n",
        "    consumer_secret = \"colocar\"\n",
        "    access_token = \"colocar\"\n",
        "    access_token_secret = \"colocar\"\n",
        "\n",
        "    # Autenticación con API de Twitter\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    # Crear un objeto API\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    # Archivos de entrada y salida\n",
        "    input_file = 'archivo_unificado.tsv'\n",
        "    output_file = 'tweets_hydrated1.tsv'\n",
        "\n",
        "    # Número máximo de tweets a revisar\n",
        "    max_tweets = 1000\n",
        "\n",
        "    # Palabra clave para filtrar los tweets\n",
        "    keywords = [\"vacunar\",\"antivacuna\",\"trombos\",\"coagulos\",\"imanes\",\"chip\",\"ADN\",\"ARN\",\"manipulan\",\"vacunas\"]\n",
        "\n",
        "    # Leer el archivo de entrada\n",
        "    df_input = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "\n",
        "    # Cargar el DataFrame existente desde un archivo o variable\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv\") as fd:\n",
        "        df_viejo = pd.read_csv(fd, delimiter=\"\\t\")\n",
        "\n",
        "    # Subset con id unicos del archivo viejo\n",
        "    existing_ids = df_viejo['User ID']\n",
        "\n",
        "    # Filtra los nuevos IDs para evitar duplicados\n",
        "    # Filtrar los IDs de los tweets que cumplen las condiciones requeridas\n",
        "    filtered_tweet_ids =df_input[(df_input['lang'] == 'es')]['tweet_id']\n",
        "    filtered_tweet_ids = [tweet_id for tweet_id in filtered_tweet_ids if tweet_id not in existing_ids]\n",
        "\n",
        "    # Obtener los IDs de los tweets únicos del archivo de entrada\n",
        "    tweet_ids = filtered_tweet_ids\n",
        "\n",
        "    # Seleccionar aleatoriamente los IDs de los tweets a revisar\n",
        "    tweet_ids_sample = random.sample(tweet_ids, min(max_tweets, len(tweet_ids)))\n",
        "\n",
        "\n",
        "    # Lista para almacenar los datos de los tweets\n",
        "    tweet_data = []\n",
        "\n",
        "    # Iterar sobre los IDs de los tweets seleccionados aleatoriamente\n",
        "    for tweet_id in tweet_ids_sample:\n",
        "        try:\n",
        "            # Obtener el tweet completo a partir del ID\n",
        "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
        "\n",
        "            # Verificar si el tweet contiene alguna de las palabras clave\n",
        "            if any(keyword in tweet.full_text for keyword in keywords):\n",
        "                # Obtener la información del tweet\n",
        "                id_user = tweet.user.id_str\n",
        "                name_user = tweet.user.screen_name\n",
        "                location = tweet.user.location\n",
        "                text = tweet.full_text\n",
        "                url = tweet.entities.get('urls')[0]['expanded_url'] if tweet.entities.get('urls') else None,\n",
        "                date = tweet.created_at.date()\n",
        "                hashtags = [tag['text'] for tag in tweet.entities.get('hashtags')]\n",
        "\n",
        "                # Agregar los datos del tweet a la lista\n",
        "                tweet_data.append([tweet_id, id_user, name_user, location, text, url, date, hashtags])\n",
        "\n",
        "        except tweepy.TweepyException as e:\n",
        "            # Manejar errores en caso de que el tweet no pueda ser encontrado o haya algún problema con la API\n",
        "            print(f\"Error al obtener el tweet con ID {tweet_id}: {e}\")\n",
        "\n",
        "    # Crear un DataFrame con los datos de los tweets\n",
        "    columns = ['Tweet ID', 'User ID', 'User Name', 'Location', 'Text', 'URL', 'Date', 'Hashtags']\n",
        "    df_output = pd.DataFrame(tweet_data, columns=columns)\n",
        "\n",
        "\n",
        "    # Guardar el DataFrame en un archivo TSV\n",
        "    #df_output.to_csv(output_file, sep='\\t', index=False)\n",
        "    # Cargar el DataFrame existente desde un archivo o variable\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv\") as fd:\n",
        "        df_viejo = pd.read_csv(fd, delimiter=\"\\t\")\n",
        "\n",
        "    # Subset con id unicos del archivo viejo\n",
        "    existing_ids = df_viejo['User ID']\n",
        "\n",
        "    # Filtra los nuevos IDs para evitar duplicados\n",
        "    new_ids = df_output[~df_output['User ID'].isin(existing_ids)]\n",
        "\n",
        "    # Concatenar los DataFrames existente y nuevo\n",
        "    df_combina = pd.concat([df_viejo, new_ids], ignore_index=True)\n",
        "\n",
        "    # PARA GUARDAR EL NUEVO ARCHIVO\n",
        "\n",
        "    # 1. Creamos la ruta y el nombre de archivo que deseas verificar y eliminar si existe\n",
        "    file_path = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "\n",
        "    # Verifica si el archivo existe en la ruta especificada\n",
        "    if os.path.exists(file_path):\n",
        "        # Elimina el archivo si existe\n",
        "        os.remove(file_path)\n",
        "        print(\"El archivo existente ha sido eliminado.\")\n",
        "    else:\n",
        "        print(\"El archivo no existe.\")\n",
        "\n",
        "    # NUEVA VEZ le decimos la ruta y el nombre de archivo para guardar el DataFrame combinado en formato TSV\n",
        "    output_file = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "\n",
        "    # Guarda el DataFrame combinado en el archivo TSV\n",
        "    df_combina.to_csv(output_file, sep='\\t', index=False)\n",
        "\n",
        "\n",
        "      # Ciclo interno para esperar 15 minutos antes de la siguiente ejecución\n",
        "    for j in range(intervalo_tiempo):\n",
        "        print(f\"Tiempo restante: {intervalo_tiempo - j} segundos\")\n",
        "        time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m0DZ_eRnZU7"
      },
      "outputs": [],
      "source": [
        "# PARA VISUALIZAR EL ARCHIVO UNIFICADO COMO DATAFRAME DE PANDA\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "pd.DataFrame(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIi2cJ1Rn3np"
      },
      "outputs": [],
      "source": [
        "# PARA CONTAR LOS TUIT DEL ARCHIVO UNIFICADO\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "\n",
        "# Contador de IDs obtenidos\n",
        "count = 0\n",
        "\n",
        "# Abrir el archivo de entrada para lectura\n",
        "with open(input_file) as f_input:\n",
        "    reader = csv.reader(f_input, delimiter='\\t')\n",
        "\n",
        "    # Ignorar la primera línea (encabezados)\n",
        "    next(reader)\n",
        "\n",
        "    # Iterar sobre las filas del archivo\n",
        "    for row in reader:\n",
        "        # Incrementar el contador por cada fila (ID)\n",
        "        count += 1\n",
        "\n",
        "print(\"Total de IDs obtenidos:\", count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbLnzij9XMUq"
      },
      "source": [
        "# Generar data final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_hMW1zR3mzp"
      },
      "outputs": [],
      "source": [
        "# PARA VER SI HAY REGISTROS DUPLICADOS\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/tuits/combined_file.tsv'\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "df1 = df[df.duplicated(keep=False)]\n",
        "df1 = df1.groupby(df1.columns.tolist()).apply(lambda x: x.index.tolist()).values.tolist()\n",
        "print (df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvferflPot9J"
      },
      "outputs": [],
      "source": [
        "# PARA ELIMINAR DUPLICADOS\n",
        "df = df.drop_duplicates(subset=['User ID'])\n",
        "\n",
        "print(\"Total registros sin duplicados\",len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-procesamiento de datos"
      ],
      "metadata": {
        "id": "L852gMtLEHh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1WFtT0d-Q6SD"
      },
      "outputs": [],
      "source": [
        "# Cargar el DataFrame existente desde un archivo o variable para poder manipularlo\n",
        "\n",
        "tweets=df\n",
        "# Convertir en string la columna texto\n",
        "tweets.Text=tweets.Text.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza"
      ],
      "metadata": {
        "id": "LvamfokrEk4j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQqk0mXP98kx"
      },
      "outputs": [],
      "source": [
        "# Esta función limpia y tokeniza el texto en palabras individuales.\n",
        "# El orden en el que se va limpiando el texto no es arbitrario.\n",
        "# El listado de signos de puntuación se ha obtenido de: print(string.punctuation) y re.escape(string.punctuation)\n",
        "# Tomada de :https://www.cienciadedatos.net/documentos/py25-text-mining-python\n",
        "\n",
        "import re\n",
        "\n",
        "def limpiar_tokenizar(texto):\n",
        "  # Se convierte todo el texto a minúsculas\n",
        "    nuevo_texto = texto.lower()\n",
        "    # Eliminación de '@' y '#'\n",
        "    nuevo_texto = re.sub(r'\\b@\\w+\\b', '', nuevo_texto)\n",
        "    #nuevo_texto = re.sub(r'\\#\\w+', '', nuevo_texto)  # Eliminación de palabras con '#'\n",
        "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
        "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
        "    # Eliminación de espacios en blanco múltiples\n",
        "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
        "    # Eliminación de signos de puntuación y caracteres no alfanuméricos\n",
        "    regex = '[^\\w\\s]'\n",
        "    nuevo_texto = re.sub(regex, '', nuevo_texto)\n",
        "    # Eliminación de números que no están seguidos de palabras clave\n",
        "    #nuevo_texto = re.sub(r\"\\b(?<!\\w)(\\d+)\\b(?!(?:\\s+(?:edad|años|dosis|vacunas|personas|pacientes|inscriptos|casos|vacunados))\\b)\", '', nuevo_texto)\n",
        "    # Eliminación de números\n",
        "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
        "    # Tokenización por palabras individuales\n",
        "    nuevo_texto = nuevo_texto.split()\n",
        "    # Reemplazar guiones bajos entre palabras por espacios\n",
        "    nuevo_texto = [item.replace('_', '') for item in nuevo_texto]\n",
        "    # Eliminación de tokens con una longitud < 2, palabras que comienzan o terminan con letras repetidas y palabras repetidas\n",
        "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1 and not re.match(r'(\\w)\\1', token) and not token.startswith('_') and not token.endswith('_')]\n",
        "\n",
        "    return nuevo_texto\n",
        "\n",
        "#Prueba\n",
        "test = \"Esto es 146.010 dosis 2as3i aah ej_emplo a_nda 60 dosis  de _l' limpieza de 6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
        "print(test)\n",
        "print(limpiar_tokenizar(texto=test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M31vqFf6mYV3"
      },
      "outputs": [],
      "source": [
        "# Esta funcion se crea para procesar el texto luego de la primera limpieza\n",
        "# de forma que se eliminen los numeros que no esten seguidos d ela palabras claves\n",
        "#-------------NOTA: AL FINAL NO SE UTILIZO EN EL ESTUDIO-----------------------------------------------------------------\n",
        "\n",
        "texto = \"Tengo 10 dosis y 5 vacunas 3 dosis a2si 3 pacientes disponibles.\"\n",
        "\n",
        "def numerizar(texto):\n",
        "    numeros = re.findall(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', texto)\n",
        "\n",
        "    for numero in numeros:\n",
        "        if re.search(rf\"{numero}\\s+(?:dosis|vacunas|personas|pacientes|inscriptos|casos|vacunados)\\b\", texto):\n",
        "            # El número está seguido de \"dosis\" o \"vacuna\", lo dejamos como está\n",
        "            continue\n",
        "        else:\n",
        "            # Sustituimos el número por una cadena vacía\n",
        "            texto = texto.replace(numero, \"\")\n",
        "\n",
        "    return texto\n",
        "numerizar(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizacion"
      ],
      "metadata": {
        "id": "KFQBU5WrEgMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXyw_7bsQlyg"
      },
      "outputs": [],
      "source": [
        "# Se aplica la función de limpieza y tokenización a cada tweet\n",
        "tweets['Texto tokenizado'] = tweets['Text'].apply(lambda x: numerizar(x)).apply(limpiar_tokenizar)\n",
        "tweets[['Text', 'Texto tokenizado']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminacion de Stop words"
      ],
      "metadata": {
        "id": "JLVACppgb53E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p587-hOshpjp"
      },
      "outputs": [],
      "source": [
        "# Obtención de listado de stopwords -articulos, conectores etc- del español\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = list(stopwords.words('spanish')) # Es un diccionario\n",
        "# Se añade la stoprword:\n",
        "stop_words.extend((\"si\",\"él\",'solo','así'))\n",
        "print(stop_words[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTsvYVW3BFbS"
      },
      "outputs": [],
      "source": [
        "# Se aplica un reset de los indices para que no pierda regitros(filas) al usar el enumerate:\n",
        "tweets=tweets.reset_index(drop=True)\n",
        "# Quitando los stopwords a la lemmatizacion\n",
        "for i, document in enumerate(tweets['Texto tokenizado']):\n",
        "    document_without_stopwords = [word for word in document if word not in stop_words]\n",
        "    tweets['Texto tokenizado'][i] = document_without_stopwords\n",
        "\n",
        "#Prueba\n",
        "tweets[['Text','Texto tokenizado']].head(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buscar terminos: una funcion para buscar un termino especifico en los textos"
      ],
      "metadata": {
        "id": "6Nu_NNUY8hOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscar el término en el texto\n",
        "def buscar_terminos(tokens, terminos):\n",
        "    for termino in terminos:\n",
        "        if termino in tokens:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "terminos=['brasil']\n",
        "#tweets[tweets['Texto lemma'].apply(lambda x: buscar_terminos(x,terminos))]"
      ],
      "metadata": {
        "id": "7A4MyhH86DEn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIkuURbyosR"
      },
      "source": [
        "# Lemmatizacion:\n",
        "Reducir las palabras a su forma base o raíz, lo que ayuda a agrupar términos que comparten una raíz común."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oVpVIWyLbn1F"
      },
      "outputs": [],
      "source": [
        "# Proceso de lemmatizacion en español usando spacy\n",
        "# Cargar el modelo de spaCy para español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Definir una función para lematizar un texto\n",
        "def lemmatize(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return lemmas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8U_-Np4LPQ"
      },
      "source": [
        "## Cambios en textos para evitar overlapping\n",
        "Cuando trabajamos con texto pasa que hay varios terminos que estan mal escrito, estos terminos se identificaron en una breve revision de la set de documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oIx9g-_xypsj"
      },
      "outputs": [],
      "source": [
        "# Creamos funcion para sustituir palabras\n",
        "\n",
        "def reemplazar_termino(lista_tokens, termino_actual, termino_nuevo):\n",
        "    return [token if token != termino_actual else termino_nuevo for token in lista_tokens]\n",
        "\n",
        "# Aplicamos a la data\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunarar','vacunar'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunacer','vacunar'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunir','vacunar'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'coronaviru','covid'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'coronavirus','covid'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunas','vacuna'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunación','vacunar'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'plandemia','pandemia'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'vacunasenchimalhuacán','municipio'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'chimalhuacán','municipio'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'venezuela','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'cuba','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'mexico','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'méxico','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'colombia','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'españa','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'argentina','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'chile','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'brasil','país'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'astrazeneco','astrazeneca'))\n",
        "tweets['Texto tokenizado']=tweets['Texto tokenizado'].apply(lambda x: reemplazar_termino(x,'él','el'))\n",
        "\n",
        "#otras palabras; coronaviru, vacunacer, coronavirus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRZ_yHCem7HY"
      },
      "outputs": [],
      "source": [
        "# Aplicacion de la funcion de lemmatizacion\n",
        "tweets['Texto lemma'] = tweets['Texto tokenizado'].apply(lambda x: lemmatize(x))\n",
        "tweets[['Text','Texto tokenizado', 'Texto lemma']].head(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KsUk3sjM1Gi"
      },
      "source": [
        "# Análisis exploratorio\n",
        "\n",
        "En Python, una de las estructuras que más facilita el análisis exploratorio es el *DataFrame* de Pandas, que es la estructura en la que se encuentra almacenada ahora la información de los tweets. Sin embargo, al realizar la *tokenización*, ha habido un cambio importante. Antes de dividir el texto, los elementos de estudio eran los tweets, y cada uno se encontraba en una fila, cumplimento así la condición de *tidy data*: una observación, una fila. Al realizar la tokenización, el elemento de estudio ha pasado a ser cada token (palabra), incumpliendo así la condición de tidy data. Para volver de nuevo a la estructura ideal se tiene que expandir cada lista de tokens, duplicando el valor de las otras columnas tantas veces como sea necesario. A este proceso se le conoce como expansión o *unnest*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9-2TlGNNhv"
      },
      "outputs": [],
      "source": [
        "# Unnest de la columna texto_tokenizado\n",
        "tweets_tidy = tweets.explode(column='Texto tokenizado')\n",
        "tweets_tidy = tweets.explode(column='Texto lemma')\n",
        "tweets_tidy = tweets_tidy.drop(columns='Text')\n",
        "tweets_tidy = tweets_tidy.rename(columns={'Texto tokenizado':'token'})\n",
        "tweets_tidy = tweets_tidy.rename(columns={'Texto lemma':'token lemma'})\n",
        "tweets_tidy.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zVHBAESU7IOt"
      },
      "outputs": [],
      "source": [
        "# Filtrado para excluir stopwords\n",
        "tweets_tidy = tweets_tidy[~(tweets_tidy[\"token lemma\"].isin(stop_words))] # eliminarlo en los token\n",
        "#tweets = tweets[~(tweets[\"Texto lemma\"].isin(stop_words))]  # eliminarlo en el texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir4PWHDnrrjk"
      },
      "outputs": [],
      "source": [
        "# Creacion de un Diccionario\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Obtener todos los tokens de la columna 'tokens' en tweets_tidy\n",
        "all_tokens = [token for sublist in tweets['Texto lemma'] if isinstance(sublist, list) for token in sublist]\n",
        "\n",
        "# Calcular la frecuencia de las palabras\n",
        "frecuencia_palabras = Counter(all_tokens)\n",
        "\n",
        "# Crear un diccionario con las palabras y su frecuencia\n",
        "diccionario_frecuencia = dict(frecuencia_palabras)\n",
        "\n",
        "# Ordenar el diccionario de manera ascendente por los valores de frecuencia\n",
        "diccionario_ordenado = dict(sorted(diccionario_frecuencia.items(), key=lambda x: x[1],reverse=True))\n",
        "\n",
        "# Crear un objeto pandas DataFrame a partir del diccionario ordenado\n",
        "df_frecuencia = pd.DataFrame(diccionario_ordenado.items(), columns=['word', 'Frecuencia'])\n",
        "\n",
        "# Imprimir el DataFrame\n",
        "print(df_frecuencia.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfLaNytznv5G"
      },
      "source": [
        "# Term Frequency e Inverse Document Frequency:\n",
        "\n",
        "Una manera sencilla de medir la importancia de un término dentro de un documento es utilizando la frecuencia con la que aparece (*tf, term-frequency*). Esta aproximación, aunque simple, tiene la limitación de atribuir mucha importancia a aquellas palabras que aparecen muchas veces aunque no aporten información selectiva. Por ejemplo, si la palabra matemáticas aparece 5 veces en un documento y la palabra página aparece 50, la segunda tendrá 10 veces más peso a pesar de que no aporte tanta información sobre la temática del documento.\n",
        "\n",
        " Para solucionar este problema se pueden ponderar los valores tf multiplicándolos por la inversa de la frecuencia con la que el término en cuestión aparece en el resto de documentos(idf). De esta forma, se consigue reducir el valor de aquellos términos que aparecen en muchos documentos y que, por lo tanto, no aportan información selectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44XJxpdLMqWG"
      },
      "source": [
        "Para calcular TF:\n",
        "\n",
        "\\begin{equation}\n",
        "TF(t,d)=\\frac{n_t}{longitud_d}\n",
        "\\end{equation}\n",
        "\n",
        "Donde de $n_t$ es el número de veces que aparece el término $t$ en el documento $d$.\n",
        "\n",
        "Para calcular IDF:\n",
        "\\begin{equation}\n",
        "IDF(t)=log(\\frac{n_d}{n_{(d,t)}})\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Donde  $n_d$ es el número total de documentos y  $n_{(d,t)}$ es el número de documentos que contienen el término  $t$.\n",
        "\n",
        "\n",
        "Para calcular TF-IDF:\n",
        "\\begin{equation}\n",
        "TF*IDF\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eED_S2JbEHCR"
      },
      "outputs": [],
      "source": [
        "# ANTES DE ELIMAR STOPWORDS\n",
        "import math\n",
        "\n",
        "# Número de veces que aparece cada término en cada tweet\n",
        "term_counts = tweets_tidy.groupby(\"token lemma\").size().reset_index(name=\"count\")\n",
        "\n",
        "# Número de IDs únicos asociados a cada término\n",
        "id_counts = tweets_tidy.groupby(\"token lemma\")[\"User ID\"].nunique().reset_index(name=\"id_count\")\n",
        "\n",
        "# Combinar los resultados en un solo DataFrame\n",
        "t_f = pd.merge(term_counts, id_counts, on=\"token lemma\")\n",
        "\n",
        "total_document = tweets_tidy[\"User ID\"].drop_duplicates().count()\n",
        "\n",
        "# Calcular el TF\n",
        "t_f['total_count'] = t_f['count'].sum()  #representa el número total de términos en el conjunto de datos.\n",
        "t_f['tf'] = t_f['count'] / t_f['total_count']\n",
        "t_f['idf']=np.log(total_document/t_f['id_count']) #---> numero de tuits\n",
        "t_f['TF-IDF']=t_f['tf']*t_f['idf']\n",
        "\n",
        "# Ordenar por TF de forma descendente\n",
        "t_f = t_f.sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "# Imprimir los resultados\n",
        "t_f.head(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDVNPnZZMztL"
      },
      "outputs": [],
      "source": [
        "#Otra forma de hacerlo mediante paqueteria de py\n",
        "\n",
        "from time import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=None,\n",
        "    max_df=0.5, #ignorar los terminos que aparecen en el 50% de los documentoss\n",
        "    min_df=5, # ignorar term que no aparecen al menos 5 documentos\n",
        "    stop_words=None,\n",
        ")\n",
        "t0 = time()\n",
        "X_tfidf = vectorizer.fit_transform(tweets_tidy['token lemma'])\n",
        "\n",
        "print(f\"Tiempor de vectorizacion : {time() - t0:.3f} s\")\n",
        "print(f\"no. muestras: {X_tfidf.shape[0]}, unicos: {X_tfidf.shape[1]}\")\n",
        "\n",
        "#Cuantificar la dispersión de la matriz como La fracción de entradas distintas de cero dividida por el número total de elementos\n",
        "print('Encontramos que alrededor de:',f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\",'las entradas de la matriz son distintas de cero')\n",
        "\n",
        "\n",
        "# Obtener las características y sus nombres\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear un DataFrame para el TF-IDF\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=features)\n",
        "\n",
        "# Calcular el promedio de TF-IDF para cada palabra\n",
        "average_tfidf = tfidf_df.mean()\n",
        "\n",
        "# Obtener las palabras con mayor valor TF-IDF\n",
        "top_words =  average_tfidf.sort_values(ascending=False)\n",
        "top_words_df = pd.DataFrame({\"token\": top_words.index, \"tf-idf\": top_words.values})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82C2QPhpGXFV"
      },
      "source": [
        "# Nube de palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySNEmbX93viT"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "\n",
        "tokens_sin_stopwords = [word for word in list(itertools.chain.from_iterable(tweets['Texto lemma'])) if word.lower() not in stop_words]\n",
        "texto = ' '.join(tokens_sin_stopwords)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7aJ71I5EJEd"
      },
      "source": [
        "# Vectorizacion con embedding:\n",
        "\n",
        "Los embeddings capturan información semántica y sintáctica sobre las palabras, lo que te permite realizar diversas tareas de procesamiento de texto utilizando estos vectores de alta dimensionalidad.\n",
        "\n",
        "Capturar el significado textual de un texto largo: Los embeddings también pueden utilizarse para capturar el significado contextual de un texto más largo, como una oración o un documento. Una forma común de hacer esto es promediando los vectores de las palabras que componen el texto. El proceso sería el siguiente:\n",
        "\n",
        "Paso 1: Para cada palabra en el texto, se obtiene su vector de embedding.\n",
        "\n",
        "Paso 2: Se promedian todos los vectores de embedding de las palabras del texto para obtener un vector promedio.\n",
        "\n",
        "Paso 3: El vector promedio resultante representa el significado contextual del texto.\n",
        "\n",
        "Al promediar los vectores de las palabras, se capturan las características semánticas del texto en su conjunto. Por ejemplo, si el texto contiene las palabras \"gato\", \"animal\" y \"doméstico\", el vector promedio capturará la idea general de un animal doméstico relacionado con los gatos.\n",
        "\n",
        "## Word2vec\n",
        " Es un modelo popular que utiliza redes neuronales para aprender embeddings de palabras. Puede entrenarse en grandes corpus de texto y genera representaciones vectoriales densas para las palabras. Word2Vec puede capturar relaciones semánticas y sintácticas entre las palabras.\n",
        "\n",
        "\n",
        "El funcionamiento básico de Word2Vec se basa en dos arquitecturas principales: Skip-gram y CBOW (Continuous Bag of Words). Ambas arquitecturas son modelos de redes neuronales que aprenden a predecir las palabras circundantes a una palabra objetivo en un contexto dado.\n",
        "\n",
        "Arquitectura Skip-gram: En esta arquitectura, el objetivo es predecir las palabras vecinas dada una palabra objetivo. Por ejemplo, si tenemos la oración \"El gato está durmiendo en la alfombra\", y elegimos \"durmiendo\" como palabra objetivo, el modelo intentará predecir las palabras vecinas como \"está\", \"en\" y \"la\". El modelo ajusta sus pesos a medida que avanza en el corpus para maximizar la probabilidad de predecir correctamente las palabras vecinas.\n",
        "\n",
        "Arquitectura CBOW: En esta arquitectura, el objetivo es predecir la palabra objetivo dadas las palabras vecinas en un contexto. Utilizando el mismo ejemplo anterior, si tenemos las palabras vecinas \"está\", \"en\" y \"la\", el modelo intentará predecir la palabra objetivo \"durmiendo\". Al igual que en Skip-gram, el modelo ajusta sus pesos para maximizar la probabilidad de predecir correctamente la palabra objetivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "193EoHDqiW6l"
      },
      "source": [
        "La **muestra negativa** se utiliza para generar ejemplos falsos al seleccionar palabras que no están en el contexto. El objetivo es que el modelo aprenda a distinguir entre palabras reales (positivas) y palabras falsas (negativas) en el contexto.\n",
        "\n",
        "La muestra negativa ayuda a mejorar la calidad de los vectores de palabras, ya que obliga al modelo a aprender a diferenciar entre palabras que están realmente relacionadas en el contexto y palabras que no lo están. Esto permite capturar mejor las relaciones semánticas y mejorar la capacidad del modelo para realizar tareas como la búsqueda de palabras similares o el análisis de similitud entre palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dn91pvxZYeDY"
      },
      "outputs": [],
      "source": [
        "#Paqueteria requerida para embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "from gensim.models import word2vec, FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx5zgLKn1nIN"
      },
      "source": [
        "\n",
        "En la biblioteca `gensim` de Python, los principales parámetros del modelo `Word2Vec` son los siguientes:\n",
        "\n",
        "- `sentences`: Los datos de entrada, que deben ser una lista de listas de palabras tokenizadas.\n",
        "- `size`: La dimensión del espacio vectorial en el que se generan los embeddings de palabras. Representa el tamaño del vector de características para cada palabra. Por defecto es 100.\n",
        "- `window`: La ventana de contexto utilizada para predecir una palabra dada una palabra de contexto. Especifica la cantidad máxima de palabras que se consideran a cada lado de la palabra objetivo. Por defecto es 5.\n",
        "- `min_count`: El umbral de frecuencia mínimo para una palabra. Las palabras que ocurren menos veces que este umbral se ignoran en el modelo. Por defecto es 5.\n",
        "- `sg`: El algoritmo utilizado para entrenar el modelo. `sg=0` corresponde al modelo CBOW (Continuous Bag of Words) y `sg=1` corresponde al modelo Skip-gram. Por defecto es 0 (CBOW).\n",
        "- `hs`: Si es 1, se utiliza la jerarquía softmax para el entrenamiento, lo que acelera el proceso de entrenamiento y mejora la calidad de los embeddings. Si es 0, se utiliza el muestreo negativo. Por defecto es 0.\n",
        "- `negative`: La cantidad de muestras negativas utilizadas para el muestreo negativo. Por defecto es 5.\n",
        "- `workers`: La cantidad de hilos de procesamiento para entrenar el modelo. Por defecto es 3.\n",
        "- `epochs`: El número de iteraciones sobre el corpus de entrenamiento. Por defecto es 5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArhhMdR3ZPDM"
      },
      "source": [
        "## Modelo vectorizador word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rBnwC5aBfEqY"
      },
      "outputs": [],
      "source": [
        "#Creamos una new data para usar en los embedding\n",
        "datos_tokenizados=tweets['Texto lemma']  #.sample(n=5000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N_SBUbaZZn0P"
      },
      "outputs": [],
      "source": [
        "#Aplicamos el word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Entrenamiento del modelo Word2Vec\n",
        "modelo = Word2Vec(datos_tokenizados,vector_size=100, sg=0,min_count=5,window=5,\n",
        "                  hs=0,epochs=5,negative=10,seed=42,workers=1)  # sg=1 para utilizar Skip-gram\n",
        "\n",
        "# Obtención del embedding de una palabra específica\n",
        "embedding = modelo.wv['vacuna']\n",
        "\n",
        "# Obtención de palabras similares a una palabra dada\n",
        "similares = modelo.wv.most_similar('vacuna')\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "modelo.save('modelo_word2vec.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zoO1BNuftmk"
      },
      "outputs": [],
      "source": [
        "#Para poder visualizar un vector del modelo\n",
        "vector = modelo.wv.get_vector(\"vacuna\")\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5lpwSPCgazU"
      },
      "outputs": [],
      "source": [
        "# Obtencion de palabras segun similitud del coseno\n",
        "similares = modelo.wv.most_similar(\"vacuna\",topn=5)\n",
        "print(similares)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oEAnLgNru9Mp"
      },
      "outputs": [],
      "source": [
        "# Esta funcion esta creada para poder obtener vectores promedios segun el texto correpsondiente\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.dim = word2vec.wv.vector_size #modificado de funcion original\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv] or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-68NxJSruJ1E"
      },
      "outputs": [],
      "source": [
        "# Aplicamos funcion anterior\n",
        "mean_embedding_vectorizer = MeanEmbeddingVectorizer(modelo)\n",
        "mean_embedded = mean_embedding_vectorizer.transform(tweets['Texto lemma'])\n",
        "tweets[\"Vector\"]=list(mean_embedded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mDu5j_gRyEIl"
      },
      "outputs": [],
      "source": [
        "#Visualizacion de data con vectores\n",
        "tweets[['Text','Texto lemma','Vector']]\n",
        "\n",
        "# Guardamos el archivo csv para siguiente paso\n",
        "tweets.to_csv('datosTFM.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rr44HYM-rNE"
      },
      "source": [
        "# Analisis de Componentes Princiales PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kLuyuLymROgJ"
      },
      "outputs": [],
      "source": [
        "# Obtener las TOP de palabras más frecuentes como una lista\n",
        "import tensorflow\n",
        "\n",
        "top_100 = t_f.nlargest(150, 'TF-IDF')['token lemma'].tolist()\n",
        "#top_100 =top_words_df.nlargest(150, 'tf-idf')['token'].tolist()\n",
        "# Matriz de vectores\n",
        "X =modelo.wv[modelo.wv.index_to_key]\n",
        "# Label de lo svectores\n",
        "words = list(modelo.wv.index_to_key)\n",
        "\n",
        "# Filtrar los vectores Word2Vec y las etiquetas para las 100 palabras más frecuentes\n",
        "vectors_top_100 = []\n",
        "labels_top_100 = []\n",
        "for i, word in enumerate(top_100):\n",
        "    word_idx = np.where(word == np.array(top_100))[0][0]  # Índice de la palabra en 'word'\n",
        "    vectors_top_100.append(X[word_idx])\n",
        "    labels_top_100.append(word)\n",
        "\n",
        "# Convertir las listas en arrays NumPy\n",
        "df_top = np.array(vectors_top_100)\n",
        "lab_top = np.array(labels_top_100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para normalizar los datos\n",
        "#----------------NO SE UTILIZO EN EL ESTUDIO-------------------------------------------\n",
        "from sklearn.preprocessing import Normalizer\n",
        "norm_df= Normalizer().fit_transform(df_top)"
      ],
      "metadata": {
        "id": "M-THPsDjB6st"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfYGUyunsvL1"
      },
      "outputs": [],
      "source": [
        "# Aplicacion del analisis PCA que nos ayude a visulizar cuantas componentes a seleccionar\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Crear objeto PCA y ajustarlo a los datos\n",
        "pca = PCA(n_components=12,random_state=42)\n",
        "pca.fit(df_top)\n",
        "\n",
        "# Obtener la varianza explicada por cada componente\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "\n",
        "# Calcular la varianza acumulada para cada número de componentes\n",
        "cumulative_variance = np.cumsum(variance_explained)\n",
        "\n",
        "# Crear un gráfico de barras para la varianza explicada\n",
        "plt.bar(range(1, len(variance_explained) + 1), variance_explained, label='Varianza Explicada', alpha=0.7)\n",
        "\n",
        "# Crear un gráfico de línea para la varianza acumulada\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='red', label='Varianza Acumulada')\n",
        "\n",
        "# Agregar etiquetas y título al gráfico\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('Porcentaje de Varianza')\n",
        "plt.title('Varianza Explicada y Varianza Acumulada por Componente')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n",
        "print(cumulative_variance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de correlacion\n",
        "df_top=pd.DataFrame(df_top)\n",
        "cor=df_top.corr()\n",
        "print(cor)"
      ],
      "metadata": {
        "id": "Eoe1x5X7zFLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XTmrBq1H71R"
      },
      "outputs": [],
      "source": [
        "# Se modela PCA y se selecciona los dos primeros componentes\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2,random_state=42) # 50% de la varianza\n",
        "result = pca.fit_transform(df_top)\n",
        "\n",
        "#create df from the pca results\n",
        "pca_df = pd.DataFrame(result,columns=['c1','c2'])  #,'c3','c4','c5'\n",
        "\n",
        "#add the words for the hover effect\n",
        "pca_df['word'] = lab_top\n",
        "pca_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GB1zzC2GBrKp"
      },
      "outputs": [],
      "source": [
        "# Preparar Dataframe para que incluya los vectores de las dos componentes obtendias con PCA\n",
        "\n",
        "#df_frecuencia_top=df_frecuencia.head(100)\n",
        "tfidf=pd.DataFrame(t_f).sort_values(by=\"TF-IDF\", ascending=False).head(150)\n",
        "#top_words_df.head(150)\n",
        "#PCA_data_top = pca_df.merge(top_words_df, how='inner', left_on='word',right_on='token')\n",
        "PCA_data_top=pca_df.merge(t_f,how='left',left_on='word',right_on='token lemma')\n",
        "\n",
        "# Para usarlos en el cluster, de forma que solo tenga los valores de los componentes principales(2)\n",
        "pca_result = pca_df[['c1','c2']].values\n",
        "pca_lab=pca_df['word'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s38mbc_LUDV"
      },
      "source": [
        "# Grafico de dispersion:\n",
        "\n",
        "Utilizo la técnica PCA para reducir la dimensionalidad de mis vectores, luego pongo los resultados y las palabras en un marco de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKulmLa1Hu8A"
      },
      "outputs": [],
      "source": [
        "# Grafico de dispersion de palabras\n",
        "# https://builtin.com/machine-learning/nlp-word2vec-python\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "N = 1000000\n",
        "\n",
        "fig = go.Figure(data=go.Scattergl(\n",
        "   x = pca_df['c1'],\n",
        "   y = pca_df['c2'],\n",
        "   mode='markers',\n",
        "   marker=dict(\n",
        "       color=np.random.randn(N), #generacion de colores aleatorios\n",
        "       colorscale='Viridis',\n",
        "       line_width=1\n",
        "   ),\n",
        "   text=pca_df['word'],\n",
        "   textposition=\"bottom center\"\n",
        "))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq3MoMeAUOpK"
      },
      "source": [
        "# Crear conglomerados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-K0PiZfAHN8"
      },
      "source": [
        "## K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkXKq3dmj2mR"
      },
      "outputs": [],
      "source": [
        "# Probamos varios numeros de cluster primero para poder determinar cual seria el optimo de mis datos\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# semilla\n",
        "s=42\n",
        "\n",
        "# Aplicar K-means con diferentes valores de n_clusters\n",
        "silueta_scores = []\n",
        "squared_errors = []\n",
        "for n in range(2, 20):\n",
        "    kmeans = KMeans(n_clusters=n,\n",
        "        max_iter=100,\n",
        "        n_init=5,\n",
        "        random_state=s)\n",
        "    kmeans.fit(pca_result)\n",
        "    labels = kmeans.labels_\n",
        "    silueta_scores.append(silhouette_score(pca_result, labels))\n",
        "    squared_errors.append(kmeans.inertia_)\n",
        "\n",
        "# Imprimir los valores de coeficiente de silueta y SSE\n",
        "for n, score, error in zip(range(2, 20), silueta_scores, squared_errors):\n",
        "    print(f\"Clusters = {n}: Silhouette Score = {score}, SSE = {error}\") #suma error cuadrado intra cluster Sum of Squared Within (SSW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QnHuKCMSRGaa"
      },
      "outputs": [],
      "source": [
        "# Crear un DataFrame para almacenar los resultados\n",
        "results_df = pd.DataFrame({\n",
        "    'Clusters': range(2, 20),\n",
        "    'Silhouette Score': silueta_scores,\n",
        "    'SSE': squared_errors\n",
        "})\n",
        "\n",
        "# Imprimir el DataFrame en formato de tabla\n",
        "#print(results_df)\n",
        "results_df.to_csv('kmeanIter.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indices de evaluación de clsuter"
      ],
      "metadata": {
        "id": "wFGZCfdUJiVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metodo del codo\n",
        "\n",
        "El método se basa en calcular la suma dentro del clúster de errores al cuadrado (WSS) para diferentes números de clústeres (k) y seleccionar el k para el cual el cambio en WSS comienza a disminuir.\n",
        "\n",
        "La idea detrás del método del codo es que la variación explicada cambia rápidamente para un pequeño número de grupos y luego se ralentiza, lo que lleva a una formación de codo en la curva. El punto de codo es el número de clústeres que podemos usar para nuestro algoritmo de agrupación."
      ],
      "metadata": {
        "id": "gMrC4vkB3D77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow Metodo de la paquteria de py\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "model = KMeans(random_state=s)\n",
        "visualizer_elbow = KElbowVisualizer(model, k=(2,20), timings= True)\n",
        "visualizer_elbow.fit(pca_result)\n",
        "\n",
        "#Aqui elaboramos el grafico a apartir del metodo anterior\n",
        "scores=visualizer_elbow.k_scores_\n",
        "centers=list(range(2,20))\n",
        "max_score_index =6 #visualizer.k_scores_.index(max(visualizer.k_scores_))\n",
        "processing_times=visualizer_elbow.k_timers_\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar el score\n",
        "ax1.plot(centers, scores, linestyle='-', marker='s', color='b',label='Índice de elbow')\n",
        "ax1.axvline(x=max_score_index, linestyle='--', color='black', label=f'k: {max_score_index}')\n",
        "ax1.set_xlabel('Número de cluster')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2, loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jX_ZUxAo2ZzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metodo de Silueta"
      ],
      "metadata": {
        "id": "RY5sIfYy_XMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "model = KMeans(random_state=s)\n",
        "visualizer_sil = KElbowVisualizer(model, k=(2,20),metric='silhouette', timings= True)\n",
        "visualizer_sil.fit(pca_result)\n",
        "\n",
        "#Aqui elaboramos el grafico de silueta a partir del metodo scores=visualizer_sil.k_scores_\n",
        "centers=list(range(2,20))\n",
        "max_score_index =2 #visualizer.k_scores_.index(max(visualizer.k_scores_))\n",
        "processing_times=visualizer_sil.k_timers_\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar el score\n",
        "ax1.plot(centers, scores, linestyle='-', marker='s', color='b',label='Índice de silueta')\n",
        "ax1.axvline(x=max_score_index, linestyle='--', color='black', label=f'k: {max_score_index}')\n",
        "ax1.set_xlabel('Número de cluster')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2,  loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vWFMrS-64dd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indice Calinski Harabasz"
      ],
      "metadata": {
        "id": "ZBVxBbKJ_TbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calinski Harabasz Score\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "model = KMeans(random_state=s)\n",
        "visualizer = KElbowVisualizer(model, k=(2,20),metric='calinski_harabasz', timings= True,locate_elbow=False)\n",
        "visualizer.fit(pca_result)\n",
        "\n",
        "#Grafico\n",
        "scores=visualizer.k_scores_\n",
        "centers=list(range(2,20))\n",
        "max_score_index =19 #visualizer.k_scores_.index(max(visualizer.k_scores_))\n",
        "processing_times=visualizer.k_timers_\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar el Davies Bouldin score\n",
        "ax1.plot(centers, scores, linestyle='-', marker='s', color='b',label='Índice de Calinski Harabasz')\n",
        "ax1.axvline(x=max_score_index, linestyle='--', color='black', label=f'k: {max_score_index}')\n",
        "ax1.set_xlabel('Número de cluster')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2,  loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q07PAWn04p2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indice de Davies Bouldin"
      ],
      "metadata": {
        "id": "khXRA5er_OBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Davies Bouldin score for K means\n",
        "import time\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "def get_kmeans_score(data, center):\n",
        "    #instantiate kmeans\n",
        "    kmeans = KMeans(n_clusters=center,random_state=s) #random evita que sea aleatorice el modelo kmean asegura reproducirlo\n",
        "\n",
        "    model = kmeans.fit_predict(pca_result,)\n",
        "\n",
        "    # Calculate Davies Bouldin score\n",
        "    score = davies_bouldin_score(pca_result, model)\n",
        "\n",
        "    return score\n",
        "\n",
        "scores = []\n",
        "centers = list(range(2, 20))\n",
        "processing_times = []\n",
        "\n",
        "for center in centers:\n",
        "    start_time = time.time()\n",
        "    scores.append(get_kmeans_score(pca_result, center))\n",
        "    end_time = time.time()\n",
        "    processing = end_time - start_time\n",
        "    processing_times.append(processing)\n",
        "\n",
        "# Encuentra el índice del score mínimo y obtén el centro correspondiente\n",
        "min_score_idx = scores.index(min(scores))\n",
        "min_score_center = centers[min_score_idx]\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar el DB score\n",
        "ax1.plot(centers, scores, linestyle='-', marker='s', color='b',label='Índice de Davies Bouldien')\n",
        "ax1.axvline(x=min_score_center, linestyle='--', color='black', label=f'k: {min_score_center}')\n",
        "ax1.set_xlabel('Número de cluster')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "#ax1.set_title('Davies Bouldin score vs. n_cluster')\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2,  loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I6O5XgjU5Gdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indice de Dunn"
      ],
      "metadata": {
        "id": "LAGMT3y7_GHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo del indice de Dunn:\n",
        "# se calcula como el cociente entre la mínima distancia entre dos puntos de diferentes clusters y\n",
        "# la máxima distancia entre dos puntos dentro del mismo cluster.\n",
        "# Un valor más alto del índice de Dunn indica una mejor separación entre los clusters.\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Rango de valores de k para probar (por ejemplo, de 2 a 20)\n",
        "k_values = range(2, 20)\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    # Calcula la dispersión intra-cluster\n",
        "    intra_cluster_distances = []\n",
        "    for cluster in np.unique(labels):\n",
        "        cluster_points = X[labels == cluster]\n",
        "        if len(cluster_points) > 1:\n",
        "            intra_distance = np.mean(cdist(cluster_points, cluster_points))\n",
        "            intra_cluster_distances.append(intra_distance)\n",
        "\n",
        "    # Calcula la dispersión inter-cluster\n",
        "    inter_cluster_distances = []\n",
        "    for i in range(len(X)):\n",
        "        for j in range(i+1, len(X)):\n",
        "            if labels[i] != labels[j]:\n",
        "                distance = np.linalg.norm(X[i] - X[j])\n",
        "                inter_cluster_distances.append(distance)\n",
        "\n",
        "    # Calcula el índice de Dunn\n",
        "    dunn_index = min(inter_cluster_distances) / max(intra_cluster_distances)\n",
        "\n",
        "    return dunn_index\n",
        "\n",
        "\n",
        "\n",
        "# Calculamos\n",
        "dunn_scores = []\n",
        "processing_times = []\n",
        "\n",
        "processing_times = []\n",
        "for k in k_values:\n",
        "    start_time = time.time()\n",
        "    kmeans = KMeans(n_clusters=k, random_state=s)\n",
        "    labels = kmeans.fit_predict(pca_result)\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    processing_times.append(processing_time)\n",
        "\n",
        "\n",
        "k_values = range(2, 20)\n",
        "# Calcular el índice de Dunn para diferentes valores de k (2 a 30)\n",
        "for k in range(2, 20):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=s)\n",
        "    clusters = kmeans.fit_predict(pca_result)\n",
        "    dunn_score = dunn_index(pca_result, clusters)\n",
        "    dunn_scores.append(dunn_score)\n",
        "print(len(dunn_scores))\n",
        "\n",
        "# Encuentra el índice del score mínimo y obtén el centro correspondiente\n",
        "mx_score_indx = dunn_scores.index(max(dunn_scores))\n",
        "mx_score = centers[mx_score_indx]\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar los resultados del índice de Dunn\n",
        "ax1.plot(range(2, 20), dunn_scores, marker='o', linestyle='-', color='b',label='Índice de Dunn')\n",
        "ax1.axvline(x=mx_score, linestyle='--', color='black', label=f'k: {mx_score}')\n",
        "ax1.set_xlabel('Número de cluster (k)')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "#ax1.title('Índice de Dunn para diferentes valores de k')\n",
        "\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2, loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IkIqv446gm9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indice de hartigan"
      ],
      "metadata": {
        "id": "8JRTfh8d_CqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Indice de hartigan log(SSB/SSW)\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def hartigan_index(X, labels):\n",
        "    # Calcula la dispersión intra-cluster\n",
        "    intra_cluster_distances = []\n",
        "    for cluster in np.unique(labels):\n",
        "        cluster_points = X[labels == cluster]\n",
        "        if len(cluster_points) > 1:\n",
        "            intra_distance = np.mean(cdist(cluster_points, cluster_points))\n",
        "            intra_cluster_distances.append(intra_distance)\n",
        "\n",
        "    # Calcula la dispersión inter-cluster\n",
        "    inter_cluster_distances = []\n",
        "    for i in range(len(X)):\n",
        "        for j in range(i+1, len(X)):\n",
        "            if labels[i] != labels[j]:\n",
        "                distance = np.linalg.norm(X[i] - X[j])\n",
        "                inter_cluster_distances.append(distance)\n",
        "\n",
        "    # Calcula el índice de Hartigan\n",
        "    hartigan_index = np.log(np.mean(inter_cluster_distances) / np.mean(intra_cluster_distances))\n",
        "\n",
        "    return hartigan_index\n",
        "\n",
        "\n",
        "# Calculamos\n",
        "hartigan_scores = []\n",
        "processing_times = []\n",
        "\n",
        "processing_times = []\n",
        "for k in k_values:\n",
        "    start_time = time.time()\n",
        "    kmeans = KMeans(n_clusters=k, random_state=s)\n",
        "    labels = kmeans.fit_predict(pca_result)\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    processing_times.append(processing_time)\n",
        "# Calcular el índice de Dunn para diferentes valores de k (2 a 20)\n",
        "for k in range(2, 20):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=s)\n",
        "    clusters = kmeans.fit_predict(pca_result)\n",
        "    hartigan_score = hartigan_index(pca_result, clusters)\n",
        "    hartigan_scores.append(hartigan_score)\n",
        "\n",
        "\n",
        "\n",
        "# Encuentra el índice del score mínimo y obtén el centro correspondiente\n",
        "mx_score_indx = hartigan_scores.index(max(hartigan_scores))\n",
        "mx_score = centers[mx_score_indx]\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Graficar los resultados del índice de Dunn\n",
        "ax1.plot(range(2, 20), hartigan_scores, marker='o', linestyle='-', color='b',label='Índice de Hartigan')\n",
        "ax1.axvline(x=mx_score, linestyle='--', color='black', label=f'k: {mx_score}')\n",
        "ax1.set_xlabel('Número de Clusters (k)')\n",
        "ax1.set_ylabel('Valor del índice')\n",
        "\n",
        "\n",
        "# Agregar eje y secundario para el tiempo de procesado\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(centers, processing_times, linestyle='--', marker='o', color='gray', label='Tiempo de Procesado')\n",
        "ax2.set_ylabel('Tiempo de Procesado (segundos)', color='black')\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Mostrar leyendas\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2,  loc='upper right',ncol=3,fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KiT7t1rOFlFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Kmean"
      ],
      "metadata": {
        "id": "f4HqbnJga7vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yET_n-VCFsBs"
      },
      "outputs": [],
      "source": [
        "# Creamos el modelo kmean final\n",
        "k=19\n",
        "for seed in range(k):\n",
        "    kmeans_vec = KMeans(\n",
        "        n_clusters=k,\n",
        "        max_iter=100,\n",
        "        n_init=5,\n",
        "        random_state=42,\n",
        "    ).fit(pca_result)\n",
        "    cluster_ids, cluster_sizes = np.unique(kmeans_vec.labels_, return_counts=True)\n",
        "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear grafico de los cluster\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "\n",
        "# Supongamos que kmeans_labels son las etiquetas de clúster devueltas por k-means\n",
        "kmeans_labels = kmeans_vec.predict(pca_result)\n",
        "# Para truncar los label de los cluster y que empiecen en 1 no en 0\n",
        "kmeans_labels = [label + 1 for label in kmeans_labels]\n",
        "\n",
        "# Recupera los clústeres únicos\n",
        "clusters = unique(kmeans_labels)\n",
        "\n",
        "# Define un mapa de colores personalizado con colores únicos para cada cluster\n",
        "custom_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'tab:blue', 'tab:orange', 'tab:green',\n",
        "                 'tab:red', 'tab:purple', 'tab:olive', 'tab:pink', 'tab:gray',  'tab:cyan', 'tab:blue', 'salmon','lime']\n",
        "\n",
        "# Crea un diccionario para mapear el número de clúster a su respectiva leyenda y color\n",
        "cluster_legend = {cluster: f'Cluster {cluster}' for cluster in clusters}\n",
        "cluster_colors = {cluster: color for cluster, color in zip(clusters, custom_colors)}\n",
        "\n",
        "# Crea el gráfico y agrega una leyenda para cada clúster\n",
        "for cluster in clusters:\n",
        "    row_ix = where(kmeans_labels == cluster)\n",
        "    plt.scatter(pca_result[row_ix, 0], pca_result[row_ix, 1], label=cluster_legend[cluster], c=cluster_colors[cluster])\n",
        "\n",
        "# Trazar los centros de clúster como \"x\"\n",
        "reduced_cluster_centers = kmeans_vec.cluster_centers_\n",
        "plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:, 1], marker='x', s=120, color='black')\n",
        "\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "\n",
        "# Muestra la leyenda fuera del gráfico y ajusta su posición\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Muestra el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_kgxs4LMSaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuRQRN1EnlfU"
      },
      "outputs": [],
      "source": [
        "# Imprimir los términos de cada cluster\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Obtener los términos de cada cluster\n",
        "clusters = kmeans_vec.labels_\n",
        "vocab =list(modelo.wv.index_to_key)\n",
        "\n",
        "# Crear un diccionario para almacenar los términos de cada cluster\n",
        "terminos_clusters = {}\n",
        "for cluster_id in range(k):\n",
        "    terminos_cluster = [lab_top[i] for i in range(len(clusters)) if clusters[i] == cluster_id]\n",
        "    terminos_clusters[cluster_id] = terminos_cluster\n",
        "\n",
        "# Imprimir\n",
        "for cluster_id, terminos in terminos_clusters.items():\n",
        "    print(f\"Cluster {cluster_id}: {terminos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtccijGcEn-_"
      },
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de silueta para cada muestra\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "PCA_data_top[\"K\"]=kmeans_vec.labels_\n",
        "\n",
        "sil_values = silhouette_samples(pca_result, kmeans_vec.labels_)\n",
        "\n",
        "# Agregar el coeficiente de silueta al DataFrame\n",
        "PCA_data_top['Silueta kmean'] = sil_values\n",
        "\n",
        "# Imprimir el coeficiente de silueta promedio para cada clúster\n",
        "for cluster_num in range(n):\n",
        "    cluster_avg = PCA_data_top[PCA_data_top['K'] == cluster_num]['Silueta kmean'].mean()\n",
        "    print(f\"Cluster {cluster_num}: Coeficiente de Silueta Promedio = {cluster_avg}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(PCA_data_top)"
      ],
      "metadata": {
        "id": "X28lYenJsFL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabla final"
      ],
      "metadata": {
        "id": "lCrdgkTw6b7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IagT0Uq-Gioq"
      },
      "outputs": [],
      "source": [
        "#PASO 1:\n",
        "\n",
        "# Agregar el coeficiente de silueta al DataFrame\n",
        "PCA_data_top['Siluetas K'] = sil_values\n",
        "\n",
        "# Agrupa los datos por el campo 'Cluster' y encuentra el índice del máximo valor en 'Conteo'\n",
        "max_ind = PCA_data_top.groupby('K')['count'].idxmax()\n",
        "\n",
        "# Obtiene los datos correspondientes al término con la mayor frecuencia en cada cluster\n",
        "terms_comun= PCA_data_top.loc[max_ind, 'word']\n",
        "t_counts = PCA_data_top.loc[max_ind, 'count']\n",
        "\n",
        "# Crear un nuevo DataFrame para mostrar los resultados\n",
        "result_km = pd.DataFrame({'Cluster': PCA_data_top.loc[max_ind, 'K'],\n",
        "                          'Most_Term': terms_comun,\n",
        "                          'Count': t_counts})\n",
        "\n",
        "# Agrupar los datos en 'pca' por el label del cluster y calcular la media de la silueta\n",
        "sil_values = PCA_data_top.groupby('K')['Siluetas K'].mean()\n",
        "\n",
        "# Unir la información del término más usado y su conteo desde 'result_df' con la información de la silueta\n",
        "result_km = result_km.merge(sil_values, left_on='Cluster', right_index=True)\n",
        "\n",
        "# Renombrar la columna de silueta a 'Silueta'\n",
        "result_km.rename(columns={'Siluetas K': 'Silueta K'}, inplace=True)\n",
        "\n",
        "# Agrupar los datos en 'pca' por el label del cluster y crear la lista de términos para cada grupo\n",
        "term_lists = PCA_data_top.groupby('K')['word'].apply(list)\n",
        "\n",
        "# Unir la lista de términos con el DataFrame 'result_df'\n",
        "result_km['Terminos'] = term_lists.values\n",
        "\n",
        "print(result_km)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PASO 2:\n",
        "\n",
        "top3=PCA_data_top.groupby('K')['count'].nlargest(3)\n",
        "\n",
        "# Calcular el top 3 de conteos para cada etiqueta de cluster\n",
        "top3_counts = PCA_data_top.groupby('K')['count'].nlargest(3)\n",
        "total=PCA_data_top.groupby('K')['count'].sum()\n",
        "\n",
        "# Obtener los top 3 términos por cluster\n",
        "top3_terms = PCA_data_top.groupby('K').apply(lambda x: x.nlargest(3, 'count')['word'].tolist())\n",
        "\n",
        "# Obtener los top 3 términos y conteos como listas\n",
        "\n",
        "top3_counts_list = []\n",
        "for group, counts in top3_counts.groupby(level=0):\n",
        "      top3_counts_list.append(list(counts.values))\n",
        "\n",
        "# Agregar las listas y el total al DataFrame original\n",
        "result_km['Top3Terminos'] =result_km['Cluster'].map(top3_terms)\n",
        "result_km['Top3Conteos'] = top3_counts_list\n",
        "result_km['TotalConteosTop3'] = top3_counts.groupby(level=0).sum().tolist()\n",
        "result_km['Totalcluster']=result_km['Cluster'].map(total)\n",
        "result_km['Peso']=result_km['TotalConteosTop3'] /result_km['Totalcluster']\n",
        "print(result_km)\n"
      ],
      "metadata": {
        "id": "-JW4T4pJhOLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9yZkEhSI5yO"
      },
      "source": [
        "# Cluster jerarquico:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "distancia=\"manhattan\"\n",
        "# Aplicar jerarquico con diferentes valores de n_clusters\n",
        "silueta_scores = []\n",
        "cohesion = []\n",
        "for n in range(2, 20):\n",
        "    kj = AgglomerativeClustering(n_clusters=n,linkage='average',metric=distancia,distance_threshold=None)\n",
        "    kluster=kj.fit_predict(pca_result)\n",
        "    labels = kj.labels_\n",
        "    silueta_scores.append(silhouette_score(pca_result, labels))\n",
        "    cohesion.append(calinski_harabasz_score(pca_result, kluster))\n",
        "\n",
        "# Imprimir los valores de coeficiente de silueta y SSE\n",
        "for n, score, error in zip(range(2, 20), silueta_scores, cohesion):\n",
        "    print(f\"Clusters = {n}: Silhouette Score = {score}, cohesion = {error}\")"
      ],
      "metadata": {
        "id": "yNIiLu4rvKrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxXOqEDgTYPl"
      },
      "outputs": [],
      "source": [
        "# Realizar el clúster jerárquico utilizando AgglomerativeClustering\n",
        "#np.random.seed (42)\n",
        "clustering = AgglomerativeClustering(n_clusters=10,linkage='average',metric=distancia,distance_threshold=None) #2.5\n",
        "cluster_labels = clustering.fit_predict(pca_result)\n",
        "\n",
        "# Calcular la silueta y la cohesión\n",
        "silhouette = silhouette_score(pca_result, cluster_labels)\n",
        "cohesion = calinski_harabasz_score(pca_result, cluster_labels)\n",
        "\n",
        "print('Numero de cluster creados:',clustering.n_clusters_)\n",
        "print(\"Silueta:\", silhouette)\n",
        "print(\"Cohesión:\", cohesion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KSsC2oRRq65"
      },
      "outputs": [],
      "source": [
        "# PCA CLUSTER: Agregar las etiquetas de clústeres al DataFrame original\n",
        "PCA_data_top['Cluster'] = clustering.labels_\n",
        "n=clustering.n_clusters_\n",
        "# Agrupar las palabras por clúster y ver las palabras asociadas a cada grupo\n",
        "clusters = {}\n",
        "for cluster_num in range(n):\n",
        "    words_in_cluster = PCA_data_top[PCA_data_top['Cluster'] == cluster_num]['word'].tolist()\n",
        "    clusters[cluster_num] = words_in_cluster\n",
        "\n",
        "# Imprimir las palabras asociadas a cada clúster\n",
        "for cluster_num, words in clusters.items():\n",
        "    print(f\"Cluster {cluster_num}: {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8krRldIOdsuj"
      },
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de silueta para cada muestra\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "silhouette_values = silhouette_samples(pca_result, cluster_labels)\n",
        "\n",
        "# Agregar el coeficiente de silueta al DataFrame\n",
        "PCA_data_top['Silhouette'] = silhouette_values\n",
        "\n",
        "# Imprimir el coeficiente de silueta promedio para cada clúster\n",
        "for cluster_num in range(n):\n",
        "    cluster_silhouette_avg = PCA_data_top[PCA_data_top['Cluster'] == cluster_num]['Silhouette'].mean()\n",
        "    print(f\"Cluster {cluster_num}: Coeficiente de Silueta Promedio = {cluster_silhouette_avg}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kQBuzRHsdKZE"
      },
      "outputs": [],
      "source": [
        "# Agregar el coeficiente de silueta al DataFrame\n",
        "PCA_data_top['Siluetas'] = silhouette_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugGwMsmfT-aQ"
      },
      "outputs": [],
      "source": [
        "# Agrupa los datos por el campo 'Cluster' y encuentra el índice del máximo valor en 'Conteo'\n",
        "max_indices = PCA_data_top.groupby('Cluster')['count'].idxmax()\n",
        "\n",
        "# Obtiene los datos correspondientes al término con la mayor frecuencia en cada cluster\n",
        "most_common_terms = PCA_data_top.loc[max_indices, 'word']\n",
        "term_counts = PCA_data_top.loc[max_indices, 'count']\n",
        "\n",
        "# Crear un nuevo DataFrame para mostrar los resultados\n",
        "result_df = pd.DataFrame({'Cluster': PCA_data_top.loc[max_indices, 'Cluster'],\n",
        "                          'Most_Term': most_common_terms,\n",
        "                          'Count': term_counts})\n",
        "\n",
        "# Agrupar los datos en 'pca' por el label del cluster y calcular la media de la silueta\n",
        "silhouette_values = PCA_data_top.groupby('Cluster')['Siluetas'].mean()\n",
        "\n",
        "# Unir la información del término más usado y su conteo desde 'result_df' con la información de la silueta\n",
        "result_df = result_df.merge(silhouette_values, left_on='Cluster', right_index=True)\n",
        "\n",
        "# Renombrar la columna de silueta a 'Silueta'\n",
        "result_df.rename(columns={'Siluetas': 'Silueta'}, inplace=True)\n",
        "\n",
        "# Agrupar los datos en 'pca' por el label del cluster y crear la lista de términos para cada grupo\n",
        "term_lists = PCA_data_top.groupby('Cluster')['word'].apply(list)\n",
        "\n",
        "# Unir la lista de términos con el DataFrame 'result_df'\n",
        "result_df['Terminos'] = term_lists.values\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4pCHHFrZ3dZ"
      },
      "source": [
        "# Dendograma y cophenetico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykyWsQ2Q0z-"
      },
      "source": [
        "El coeficiente cophenético mide la correlación entre las distancias originales entre los puntos y las distancias copheneticas, que son las distancias entre los puntos en el dendrograma resultante del clustering jerárquico.\n",
        "\n",
        "El coeficiente cophenético tiene valores entre 0 y 1, donde un valor más cercano a 1 indica una mejor estructura de clustering jerárquico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DB3E0OeDQuI"
      },
      "source": [
        "En una dimension de 100 con CBOW y muestreo negativo y ventana en 5 tenemos este resultado... pero si bajamos la dimension, y la ventana de muestre negativo tenemos resultados en los cuales predomina un solo color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9fY7SbrbKiA"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Realizar el clúster jerárquico utilizando linkage para obtener las distancias\n",
        "Z =linkage(pca_result, method='average',metric='cityblock')\n",
        "\n",
        "# corte óptimo\n",
        "optimal_threshold = 3\n",
        "optimal_threshold2 = 1\n",
        "# Crear el dendrograma\n",
        "plt.figure(figsize=(10, 10))\n",
        "dendrogram(Z, labels=pca_lab ,leaf_font_size=5,orientation='right')\n",
        "plt.title('Dendrograma del Clúster Jerárquico:200 palabras, enlace:avg')\n",
        "plt.xlabel('Distancia')\n",
        "plt.ylabel('Palabras')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid()\n",
        "\n",
        "# Agregar la línea punteada para indicar el corte óptimo\n",
        "plt.axvline(x=optimal_threshold, color='red', linestyle='dashed')\n",
        "plt.axvline(x=optimal_threshold2, color='blue', linestyle='dashed')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snt3s08AQTX7"
      },
      "outputs": [],
      "source": [
        "# Calcular las distancias copheneticas\n",
        "c,coph_dists = cophenet(Z, pdist(pca_result))\n",
        "#print((coph_dists))\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etiquetado manual"
      ],
      "metadata": {
        "id": "hvdSs8YuZTyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraccion de muestras"
      ],
      "metadata": {
        "id": "lvUV0r3JNvL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos esta funcion que solo hace limpieza de texto no genera token para\n",
        "# poder hacer la matriz y grafico de coocurrencia\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "  # Se convierte todo el texto a minúsculas\n",
        "    nuevo_texto = texto.lower()\n",
        "    # Eliminación de '@' y '#'\n",
        "    nuevo_texto = re.sub(r'\\b@\\w+\\b', '', nuevo_texto)\n",
        "    #nuevo_texto = re.sub(r'\\#\\w+', '', nuevo_texto)  # Eliminación de palabras con '#'\n",
        "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
        "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
        "    # Eliminación de espacios en blanco múltiples\n",
        "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
        "    # Eliminación de signos de puntuación y caracteres no alfanuméricos\n",
        "    regex = '[^\\w\\s]'\n",
        "    nuevo_texto = re.sub(regex, '', nuevo_texto)\n",
        "    # Eliminación de números que no están seguidos de palabras clave\n",
        "    #nuevo_texto = re.sub(r\"\\b(?<!\\w)(\\d+)\\b(?!(?:\\s+(?:edad|años|dosis|vacunas|personas|pacientes|inscriptos|casos|vacunados))\\b)\", '', nuevo_texto)\n",
        "    # Eliminación de números\n",
        "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
        "\n",
        "    return nuevo_texto\n",
        "\n",
        "\n",
        "# Funcion para quitar stopwords del texto que creamos para la coocurrencia\n",
        "\n",
        "def quitar_stop_words(texto):\n",
        "    palabras = texto.split()\n",
        "    palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]\n",
        "    texto_filtrado = ' '.join(palabras_filtradas)\n",
        "    return texto_filtrado\n",
        "\n",
        "\n",
        "\n",
        "# Nueva Base de datos para trabajar: Se aplica la función de limpieza y tokenización a cada tweet en formato texto no TOKEN\n",
        "BD=tweets\n",
        "BD['Text'] = BD['Text'].apply(lambda x: limpiar_texto(x))\n",
        "# Ejemplo de uso\n",
        "BD['texto_filtrado'] = BD['Text'].apply(lambda x: quitar_stop_words(x))\n",
        "textos=BD['texto_filtrado'].sample(n=200, random_state=42)"
      ],
      "metadata": {
        "id": "O5nhB1vFD8ts"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "EzXZof-rYHXW"
      },
      "outputs": [],
      "source": [
        "#Se usan los 3 primeros terminos de los cluster para en la columna 'Text'\n",
        "# en este paso se busca obtener archivos csv para proceder a la revision manual\n",
        "\n",
        "#terminos = ['caso', 'ahora', 'medico']\n",
        "#terminos = ['astrazeneca', 'recibir', 'semana']\n",
        "#terminos = ['salud', 'ser', 'población']\n",
        "#terminos = ['vacuna', 'país', 'hacer']\n",
        "#terminos = ['decir', 'pandemia', 'aquí']\n",
        "#terminos = ['millón', 'ir', 'vacunar él']\n",
        "#terminos = ['covid', 'gobierno','morir']\n",
        "#terminos= ['vacuna', 'vacuno', 'dosis']\n",
        "#terminos=['semana', 'presidente', 'hospital', 'ninguno']\n",
        "#terminos=['salud', 'población', 'gente', 'plan', 'virus', 'uso', 'muerte', 'covax', 'clínico', 'pueblo', 'ministro', 'mismo']\n",
        "#terminos=['pandemia', 'aquí', 'menos', 'mañana']\n",
        "#terminos=['mayor', 'pfizer', 'covax']\n",
        "#terminos=['vacunar', 'aquí', 'nacional']\n",
        "#terminos=['año', 'municipio', 'hacer']\n",
        "#terminos=['poder', 'iniciar', 'jornada']\n",
        "#terminos=['segundo', 'vía', 'plan']\n",
        "#terminos=['salud', 'población', 'presidente']\n",
        "#terminos=['vacuna', 'primero']\n",
        "#terminos=['dosis', 'nuevo', 'través']\n",
        "terminos=['anticuerpo', 'grupo', 'punto']\n",
        "# Filtrar las filas que contienen los términos\n",
        "#tweets_filtrados = df[df[\"Texto lemma\"].str.contains('|'.join(terminos))]\n",
        "\n",
        "# Filtrar las listas de tokens que contienen los términos específicos\n",
        "tweets_filtrados = BD[BD['Texto lemma'].apply(lambda x: buscar_terminos(x,terminos))]\n",
        "\n",
        "cluster_13=tweets_filtrados[['User ID','Text','Texto lemma']].sample(frac=0.3,replace=False,random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Clasificacion"
      ],
      "metadata": {
        "id": "mD4zP19fFAfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar 5 mil twwet como submuestra\n",
        "sample_tweets=BD[['User ID','Text','Texto lemma']].sample(frac=0.5,replace=False,random_state=42)"
      ],
      "metadata": {
        "id": "VA0JREv_Fj__"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de listas de tokens de tuits\n",
        "tuit_tokens = tweets['Texto lemma']\n",
        "\n",
        "# Función para obtener el vector promedio de un tuit\n",
        "def obtener_vector_promedio(modelo, tokens):\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in modelo.wv.key_to_index:\n",
        "            vectors.append(modelo.wv[token])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(modelo.wv.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
        "\n",
        "# Calcular los vectores medios para todos los tuits\n",
        "vectores_medios = [obtener_vector_promedio(modelo, tokens) for tokens in tuit_tokens]\n",
        "\n",
        "# Asignamos a la base de dtaos\n",
        "tweets['vec']=(vectores_medios)\n",
        "\n",
        "# lo volvemos array porque esta en forma de lista\n",
        "tweets['vec']=np.array(vectores_medios)"
      ],
      "metadata": {
        "id": "KbgXb4DHPAW-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creacion de etiquetas"
      ],
      "metadata": {
        "id": "hAEkyrJIPdUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Creacion de data para vector y clasificacion\n",
        "term = {\n",
        "    'cluster': [\n",
        "        ['caso', 'ahora', 'medico'],\n",
        "        ['astrazeneca', 'recibir', 'semana'],\n",
        "        ['dosis', 'nuevo', 'través'],\n",
        "        ['adulto', 'gobierno', 'decir'],\n",
        "        ['segundo', 'vía', 'plan'],\n",
        "        ['poder', 'iniciar', 'jornada'],\n",
        "        ['persona', 'ir', 'personal'],\n",
        "        ['salud', 'población', 'presidente']\n",
        "    ],\n",
        "    'etiquetas': [\n",
        "        'GravedadPercibida',\n",
        "        'BarreraPercibida',\n",
        "        'BeneficioPercibida',\n",
        "        'Vulnerabilidad',\n",
        "        'SuceptibilidadPercibida',\n",
        "        'JornadasVacunacion',\n",
        "        'PersonalMedicoFrente',\n",
        "        'RiesgoSaludNacional'\n",
        "    ]\n",
        "}\n",
        "\n",
        "terminos=pd.DataFrame(term)\n",
        "\n",
        "# Tu DataFrame original con 5000 tuits\n",
        "sample_tweets = BD[['User ID', 'Text', 'Texto lemma','vec']].sample(frac=0.5, replace=False, random_state=42)\n",
        "\n",
        "# Lista para guardar los User ID que ya se han asignado a un cluster\n",
        "user_ids_asignados = []\n",
        "\n",
        "# Diccionario para almacenar los clusters\n",
        "clusters = {}\n",
        "\n",
        "# Iterar a través de los clusters definidos\n",
        "for i, cluster_terms in enumerate(term['cluster'], start=1):\n",
        "    cluster_name = f'cluster{i}'\n",
        "\n",
        "    # Filtrar los tuits para los términos del cluster actual\n",
        "    cluster_data = sample_tweets[sample_tweets['Texto lemma'].apply(lambda x: buscar_terminos(x, cluster_terms))]\n",
        "\n",
        "    # Filtrar los tuits que aún no han sido asignados a ningún cluster\n",
        "    cluster_data = cluster_data[~cluster_data['User ID'].isin(user_ids_asignados)]\n",
        "\n",
        "    # Agregar el campo 'etiqueta' con el nombre del cluster\n",
        "    cluster_data['etiqueta'] = cluster_name\n",
        "\n",
        "    # Agregar los User ID de este cluster a la lista de IDs asignados\n",
        "    user_ids_asignados.extend(cluster_data['User ID'])\n",
        "\n",
        "    # Guardar el cluster en el diccionario\n",
        "    clusters[cluster_name] = cluster_data\n",
        "\n",
        "# Concatenar los DataFrames de todos los clusters en uno solo\n",
        "final_df = pd.concat(clusters.values(), ignore_index=True)\n",
        "\n",
        "# Imprimir tamaños de los clusters\n",
        "for cluster_name, cluster_data in clusters.items():\n",
        "    print(f\"Tamaño de {cluster_name}: {cluster_data.shape}\")\n",
        "\n",
        "# Imprimir tamaño del DataFrame final\n",
        "print(f\"Tamaño del DataFrame final: {final_df.shape}\")\n"
      ],
      "metadata": {
        "id": "LqtId_C0cFS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('DataK.csv',index=False,sep=';',encoding='utf-8') #crear arcvivo csv"
      ],
      "metadata": {
        "id": "p0InXeK7k4Us"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Contar duplicados en texto\n",
        "rep=final_df[final_df.duplicated(subset=['Text'])]\n",
        "\n",
        "#Quitar duplicados\n",
        "data=final_df.drop_duplicates(subset=['Text'])\n",
        "\n",
        "print('Hay un total de',len(rep),'duplicados')"
      ],
      "metadata": {
        "id": "GmUmEsK9PSQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creacion de dummies"
      ],
      "metadata": {
        "id": "M9ShqMlRPaIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear variables dummy\n",
        "variables_dummy = pd.get_dummies(data[\"etiqueta\"], prefix=\"Categoría\")\n",
        "\n",
        "# Concatenar las variables dummy al DataFrame original\n",
        "data = pd.concat([data, variables_dummy], axis=1)\n",
        "\n",
        "# Imprimir el DataFrame resultante\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-Hv3iqmBPTLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estetica para la visualizacion de la data\n",
        "data=data.sort_values(by='User ID',ascending=True)"
      ],
      "metadata": {
        "id": "SjWzXkMQPlo1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separamos los datos en una data por etiqueta\n",
        "\n",
        "#CLUSTER 1:'GravedadPercibida',\n",
        "Gravedad=data[['Categoría_cluster1','vec']]\n",
        "\n",
        "#CLUSTER 2: 'BarreraPercibida'\n",
        "Barrera=data[['Categoría_cluster2','vec']]\n",
        "\n",
        "#CLUSTER 3: BeneficioPercibida',\n",
        "\n",
        "Beneficio=data[['Categoría_cluster3','vec']]\n",
        "\n",
        "#CLUSTER 4: Vulnerabilidad',\n",
        "Vulnerabilidad=data[['Categoría_cluster4','vec']]\n",
        "\n",
        "#CLUSTER 5: SuceptibilidadPercibida'\n",
        "\n",
        "Suceptibilidad=data[['Categoría_cluster5','vec']]\n",
        "\n",
        "#CLUSTER 6:'JornadasVacunacion'\n",
        "\n",
        "JornadasVacunacion=data[['Categoría_cluster6','vec']]\n",
        "\n",
        "#CLUSTER 7:'PersonalMedicoFrente'\n",
        "\n",
        "PersonalMedico=data[['Categoría_cluster7','vec']]\n",
        "\n",
        "#CLUSTER 8: 'RiesgoSaludNacional'\n",
        "\n",
        "RiesgoSalud=data[['Categoría_cluster8','vec']]"
      ],
      "metadata": {
        "id": "FwSyp5HFPuQ0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una variable de etiquetas con todos los cluster\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Supongamos que tienes un DataFrame llamado df y la columna etiqueta contiene tus etiquetas originales\n",
        "etiquetas_originales = data['etiqueta']\n",
        "# Crear una instancia de LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "# Ajustar y transformar las etiquetas originales a valores numéricos\n",
        "etiquetas_numericas = label_encoder.fit_transform(etiquetas_originales)\n",
        "# Crear una nueva columna en el DataFrame con las etiquetas numéricas\n",
        "data['etiqueta_todo'] = etiquetas_numericas"
      ],
      "metadata": {
        "id": "3Oie5HwSP79U"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de clasficacion"
      ],
      "metadata": {
        "id": "9oahxHgCP9Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "PnfpgsvrQBow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividir el dataset en entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42) #semilla\n",
        "\n",
        "X = Gravedad[['vec']]\n",
        "y = Gravedad['Categoría_cluster1'] #.map({1: 'Positivo', 0: 'Negativo'})\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42)"
      ],
      "metadata": {
        "id": "BlKLebS-QGlB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Es el número de árboles en el bosque\n",
        "n_estimators=[100,300,500]\n",
        "# El número máximo de características a considerar al buscar la mejor división\n",
        "max_features = ['auto']\n",
        "#Definición: La profundidad máxima de los árboles en el bosque.\n",
        "#Función: Un valor más alto permite que los árboles sean más profundos y por lo tanto, más complejos, lo que puede llevar al sobreajuste.\n",
        "# Un valor más bajo limita la profundidad de los árboles y puede llevar a un subajuste.\n",
        "max_depth = [20,25,30,35]\n",
        "# El número mínimo de muestras requeridas para dividir un nodo interno\n",
        "min_samples_split = [2]\n",
        "# El número mínimo de muestras requeridas en un nodo hoja\n",
        "min_samples_leaf = [1]\n",
        "# Metodo de de seleccion de muestras para cada arbol\n",
        "bootstrap = [True]\n",
        "\n",
        "# Criterion\n",
        "criterion=['gini']\n",
        "random_grid = {'n_estimators':n_estimators,\n",
        "               'bootstrap':bootstrap,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "                #'max_feature':max_features,\n",
        "                'criterion': criterion}"
      ],
      "metadata": {
        "id": "OgwbRNUUQNF7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Corremos el seacrh de parametros\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "rf_base = RandomForestClassifier() #class_weight='balanced_subsample'\n",
        "rf_random = RandomizedSearchCV(estimator = rf_base,\n",
        "                               n_iter = 10,\n",
        "                               param_distributions=random_grid,\n",
        "                               cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123),\n",
        "                               verbose=2,\n",
        "                               random_state=42,\n",
        "                               )\n",
        "\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8iVEsJVUQVsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelizacion final dado los best parametros\n",
        "\n",
        "best_params=rf_random.best_params_ #mejores parametros seleccionados\n",
        "\n",
        "# Crear un diccionario de pesos personalizado\n",
        "#pesos_personalizados = {\n",
        "#    0: 0.2,  # Peso para la clase 0\n",
        "#    1: 5.5,  # Peso para la clase 1\n",
        "#}\n",
        "\n",
        "modelo_final=RandomForestClassifier(**best_params,oob_score=True,class_weight='balanced')  #class_weight='balanced'\n",
        "modelo_final.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7JUtdawFQWzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de confusion"
      ],
      "metadata": {
        "id": "OkrxrKqjQnen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de confusion de datos de prueba\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "preds = modelo_final.predict(X_test)\n",
        "confu_matriz=confusion_matrix(y_true=y_test, y_pred=preds,labels=[0,1]) #,normalize='true'\n",
        "print(confu_matriz)"
      ],
      "metadata": {
        "id": "WF1kXSlsQl_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir las etiquetas utilizando los datos de entrenamiento\n",
        "y_pred_train = modelo_final.predict(X_train)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "confusion_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "confusion_matrix_train"
      ],
      "metadata": {
        "id": "pUpF8BghQug5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir un informe de clasificación para obtener más detalles\n",
        "report = classification_report(y_test, preds)\n",
        "print(\"\\nInforme de Clasificación:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "AVAPseMIQ3gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la sensibilidad y especificidad\n",
        "tn, fp, fn, tp = confu_matriz.ravel()\n",
        "sensibilidad = tp / (tp + fn)\n",
        "especificidad = tn / (tn + fp)\n",
        "print(\"\\nSensibilidad (TPR):\", sensibilidad)\n",
        "print(\"Especificidad (TNR):\", especificidad)"
      ],
      "metadata": {
        "id": "Qkn1H7XgQ586"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Curva ROC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "\n",
        "\n",
        "y_pred_proba = modelo_final.predict_proba(X_test)[:, 1]  # Probabilidades de la clase positiva\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rsFVulSpQ_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prueba de invarianza segun Sokolova paper:\n",
        " https://sci-hub.se/10.1016/j.ipm.2009.03.002"
      ],
      "metadata": {
        "id": "r944Ady3SOS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Medida de invarianza #1\n",
        "confusion_matrix(y_true=y_test, y_pred=preds,labels=[1,0])"
      ],
      "metadata": {
        "id": "uaSV8qH3RHNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Segunda medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio en tn (por ejemplo, aumentar en 1)\n",
        "tn_new = tn + 1\n",
        "\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_segunda = custom_measure(tp, fp, tn_new, fn)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_segunda:\n",
        "    print(\"La medida es invariante con respecto a tn.\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a tn.\")\n",
        "    print(original, medida_segunda)\n"
      ],
      "metadata": {
        "id": "KZ_jc9WxSVOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tercera medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio en tn (por ejemplo, aumentar en 1)\n",
        "tp_new = tp + 1\n",
        "\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_tercera = custom_measure(tp_new, fp, tn, fn)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_tercera:\n",
        "    print(\"La medida es invariante con respecto a tp.\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a tp.\")\n",
        "    print(original, medida_tercera)"
      ],
      "metadata": {
        "id": "BsAM2w8JSjpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuarta medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio en tn (por ejemplo, aumentar en 1)\n",
        "fn_new = fn + 1\n",
        "\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_cuarta = custom_measure(tp, fp, tn, fn_new)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_cuarta:\n",
        "    print(\"La medida es invariante con respecto a tp.\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a tp.\")\n",
        "    print(original, medida_cuarta)"
      ],
      "metadata": {
        "id": "veODO1o_Sw0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quinta medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio en tn (por ejemplo, aumentar en 1)\n",
        "fp_new = fp +1\n",
        "\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_quinta = custom_measure(tp, fp_new, tn, fn)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_quinta:\n",
        "    print(\"La medida es invariante con respecto a tp.\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a tp.\")\n",
        "    print(original, medida_quinta)"
      ],
      "metadata": {
        "id": "cPGpfpa0S2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sexta medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio multiplicar por un mismo escalar\n",
        "k1=2\n",
        "k2=3\n",
        "tp_6=tp*k1\n",
        "fp_6=fp*k1\n",
        "tn_6=tn*k1\n",
        "fn_6=fn*k1\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_sexta = custom_measure(tp_6, fp_6, tn_6, fn_6)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_sexta:\n",
        "    print(\"La medida es invariante con respecto a new\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a new:\")\n",
        "    print(original, medida_sexta)"
      ],
      "metadata": {
        "id": "nJNFi1tuS7Mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b546e1-0c2d-45db-c7fe-181eb1027ea3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La medida es invariante con respecto a new\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Septima medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio multiplicar por un mismo escalar\n",
        "k1=2\n",
        "k2=3\n",
        "tp_7=tp*k1\n",
        "fp_7=fp*k1\n",
        "tn_7=tn*k2\n",
        "fn_7=fn*k2\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_septima = custom_measure(tp_7, fp_7, tn_7, fn_7)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_septima:\n",
        "    print(\"La medida es invariante con respecto a new\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a new:\")\n",
        "    print(original, medida_septima)"
      ],
      "metadata": {
        "id": "rlwozb4fS_5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf86b1d-0f67-46ce-8975-5c86b8f2e9a7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La medida no es invariante con respecto a new:\n",
            "0.8394366197183099 0.8636216391318432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Octava medida de invarianza\n",
        "\n",
        "# Paso 1: Crear una función para calcular la medida\n",
        "def custom_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn)\n",
        "    return accur\n",
        "\n",
        "# Paso 2b: Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, preds)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Paso 2c: Calcular la medida original\n",
        "original = custom_measure(tp, fp, tn, fn)\n",
        "\n",
        "# Paso 2d: Simular un cambio multiplicar por un mismo escalar\n",
        "k1=2\n",
        "k2=3\n",
        "tp_8=tp*k1\n",
        "fp_8=fp*k2\n",
        "tn_8=tn*k2\n",
        "fn_8=fn*k1\n",
        "# Paso 2e: Calcular la medida con el nuevo tn\n",
        "medida_octava = custom_measure(tp_8, fp_8, tn_8, fn_8)\n",
        "\n",
        "# Paso 2f: Comparar los resultados\n",
        "if original == medida_octava:\n",
        "    print(\"La medida es invariante con respecto a new\")\n",
        "else:\n",
        "    print(\"La medida no es invariante con respecto a new:\")\n",
        "    print(original, medida_octava)"
      ],
      "metadata": {
        "id": "ik79831kTEWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a7f361-757c-4158-e9ac-cc42cf039140"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La medida no es invariante con respecto a new:\n",
            "0.8394366197183099 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para obtener todas las medidas antes generadas\n",
        "def all_measure(tp, fp, tn, fn):\n",
        "    accur=(tp + tn) / (tp + fp + tn + fn),\n",
        "    precision=tp/(tp+fp),\n",
        "    sensible=tp/(tp+fn),\n",
        "    especific=tn/(fp+tn),\n",
        "    return [accur,precision,sensible,especific]\n",
        "\n",
        "# Calcular original\n",
        "tn, fp, fn, tp = confu_matriz.ravel()\n",
        "origin=all_measure(tp, fp, tn, fn)\n",
        "# calcular nueva\n",
        "\n",
        "resultados2=all_measure(tp, fp, tn_new, fn)\n",
        "resultados3=all_measure(tp_new, fp, tn, fn)\n",
        "resultados4=all_measure(tp, fp, tn, fn_new)\n",
        "resultados5=all_measure(tp, fp_new, tn, fn)\n",
        "resultados6=all_measure(tp_6, fp_6, tn_6, fn_6)\n",
        "resultados7=all_measure(tp_7, fp_7, tn_7, fn_7)\n",
        "resultados8=all_measure(tp_8, fp_8, tn_8, fn_8)\n",
        "\n",
        "print(\"Accuracy:\",origin[0], resultados2[0],resultados3[0],resultados4[0],resultados5[0],resultados6[0],resultados7[0],resultados8[0])\n",
        "print(\"Precision:\",origin[1], resultados2[1],resultados3[1],resultados4[1],resultados5[1],resultados6[1],resultados7[1],resultados8[1])\n",
        "print(\"Sensibilidad:\", origin[2],resultados2[2],resultados3[2],resultados4[2],resultados5[2],resultados6[2],resultados7[2],resultados8[2])\n",
        "print(\"Especificidad:\",origin[3], resultados2[3],resultados3[3],resultados4[3],resultados5[3],resultados6[3],resultados7[3],resultados8[3])\n"
      ],
      "metadata": {
        "id": "x_GywY3_TJKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8625063-c7f5-4cd0-963c-721fec15af60"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: (0.8394366197183099,) (0.8395872420262664,) (0.8395872420262664,) (0.8386491557223265,) (0.8386491557223265,) (0.8394366197183099,) (0.8636216391318432,) (0.86,)\n",
            "Precision: (0.14814814814814814,) (0.14814814814814814,) (0.1559633027522936,) (0.14814814814814814,) (0.14678899082568808,) (0.14814814814814814,) (0.14814814814814814,) (0.1038961038961039,)\n",
            "Sensibilidad: (0.16842105263157894,) (0.16842105263157894,) (0.17708333333333334,) (0.16666666666666666,) (0.16842105263157894,) (0.16842105263157894,) (0.11895910780669144,) (0.16842105263157894,)\n",
            "Especificidad: (0.9051546391752577,) (0.9052523171987642,) (0.9051546391752577,) (0.9051546391752577,) (0.9042224510813595,) (0.9051546391752577,) (0.9347054648687012,) (0.9051546391752577,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo SVM"
      ],
      "metadata": {
        "id": "YzMBOIrRUOgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividir el dataset en entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42) #semilla\n",
        "\n",
        "X = Gravedad[['vec']]\n",
        "y = Gravedad['Categoría_cluster1'] #.map({1: 'Positivo', 0: 'Negativo'})\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42)"
      ],
      "metadata": {
        "id": "lcua5cQ_TTBq"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar search para el modelo SVM\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "model = SVC(kernel='poly')\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'degree': [2, 3, 4],\n",
        "    'coef0': [0, 1, 2]\n",
        "}\n",
        "#param_grid = {'C': [0.001,0.01,0.1,1],\n",
        "             # 'kernel': ['linear'],\n",
        "             # 'gamma': ['scale',0.01,0.5,1]}\n",
        "\n",
        "grid_search = RandomizedSearchCV(model,\n",
        "                                 param_distributions=param_grid,\n",
        "                                 n_iter=10,\n",
        "                                 cv=5,\n",
        "                                 scoring='accuracy',\n",
        "                                 random_state=42,\n",
        "                                 verbose=2)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UhGpT3AjTadP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo con los best parametros\n",
        "\n",
        "#Seleccionar mejores param\n",
        "best_params = grid_search.best_params_\n",
        "#Crear modelo con mejores param\n",
        "best_model = SVC(**best_params,probability=True,class_weight='balanced')\n",
        "#Correr modelo final\n",
        "best_model.fit(X_train, y_train)\n",
        "accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Precisión del modelo en datos de prueba:\", accuracy)"
      ],
      "metadata": {
        "id": "_Z3acLYiTjYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model.score(X_train, y_train)) #entrenamiento\n",
        "print(best_model.score(X_test, y_test)) #test"
      ],
      "metadata": {
        "id": "dChppFW-TrFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de confusion"
      ],
      "metadata": {
        "id": "e-ykR5WsT1tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de confusion en data test\n",
        "y_pred=best_model.predict(X_test)\n",
        "# Calcular la matriz de confusión\n",
        "conf_matriz_svc = confusion_matrix(y_test, y_pred)\n",
        "conf_matriz_svc"
      ],
      "metadata": {
        "id": "-kry4VvMTvbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir las etiquetas utilizando los datos de entrenamiento\n",
        "y_pred_train = best_model.predict(X_train)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "confusion_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "confusion_matrix_train"
      ],
      "metadata": {
        "id": "-3DDgE2dT5Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# También puedes imprimir un informe de clasificación para obtener más detalles\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\nInforme de Clasificación:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "vlaCTcART8Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la sensibilidad y especificidad\n",
        "tn, fp, fn, tp = conf_matriz_svc.ravel()\n",
        "sensibilidad = tp / (tp + fn)\n",
        "especificidad = tn / (tn + fp)\n",
        "print(\"\\nSensibilidad (TPR):\", sensibilidad)\n",
        "print(\"Especificidad (TNR):\", especificidad)"
      ],
      "metadata": {
        "id": "z4Az-1QqUAXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curva ROC"
      ],
      "metadata": {
        "id": "Q2kEVtQqUYRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Curva ROC SVM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "\n",
        "\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilidades de la clase positiva\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fSevrAz9UHFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}